# üöÄ Retrieval Plugin Optimization Guide

Ce guide explique comment optimiser les performances du retrieval plugin sur diff√©rentes plateformes.

## Table des mati√®res

1. [Quick Start](#quick-start)
2. [Platform-Specific Optimizations](#platform-specific-optimizations)
3. [Advanced Optimizations](#advanced-optimizations)
4. [Benchmarking](#benchmarking)
5. [Troubleshooting](#troubleshooting)

---

## Quick Start

### 1. Auto-detect et configurer les optimisations

```bash
python optimize_platform.py
```

Ce script va :
- D√©tecter votre plateforme (Mac Silicon, CUDA, CPU)
- G√©n√©rer un fichier `.env.optimized` avec les param√®tres optimaux
- Afficher des recommandations sp√©cifiques

### 2. Appliquer les optimisations

```bash
# Backup de votre configuration actuelle
cp .env .env.backup

# Appliquer les optimisations
cp .env.optimized .env

# Red√©marrer le serveur
poetry run start
```

### 3. Mesurer les gains

```bash
python benchmark_embeddings.py
```

---

## Platform-Specific Optimizations

### üçé Mac Silicon (M1/M2/M3/M4)

**Configuration automatique** :
- Device: `mps` (Metal Performance Shaders)
- Batch size: `32` (optimal pour MPS)
- FP16: `false` (MPS a des bugs avec FP16)

**Gains attendus** : **5-10x plus rapide** vs CPU

**Configuration manuelle** (.env):
```bash
EMBEDDING_DEVICE=mps
EMBEDDING_BATCH=32
EMBEDDING_FP16=false
EMBEDDING_MAX_LEN=8192
EMBEDDING_CACHE_SIZE=2000

RERANK_DEVICE=mps
RERANK_ENABLE=true
RERANK_K=5
RERANK_FINAL_N=3

# Optimisations MPS
PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
PYTORCH_ENABLE_MPS_FALLBACK=1
```

**Installation PyTorch pour Mac Silicon** :
```bash
pip3 install torch torchvision torchaudio
```

### üêß Linux avec GPU NVIDIA

**Configuration automatique** :
- Device: `cuda:0`
- Batch size: `64` (CUDA peut g√©rer des gros batches)
- FP16: `true` (tr√®s rapide sur CUDA)

**Gains attendus** : **10-50x plus rapide** vs CPU

**Configuration manuelle** (.env):
```bash
EMBEDDING_DEVICE=cuda:0
EMBEDDING_BATCH=64
EMBEDDING_FP16=true
EMBEDDING_MAX_LEN=8192
EMBEDDING_CACHE_SIZE=2000

RERANK_DEVICE=cuda:0
RERANK_ENABLE=true
RERANK_K=10
RERANK_FINAL_N=5

# Optimisations CUDA
CUDA_LAUNCH_BLOCKING=0
TORCH_CUDNN_V8_API_ENABLED=1
```

**Installation PyTorch pour CUDA** :
```bash
# CUDA 11.8
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### üíª CPU Fallback

**Configuration automatique** :
- Device: `cpu`
- Batch size: `16` (petit pour √©viter surcharge m√©moire)
- Max length: `4096` (r√©duit pour CPU)

**Gains attendus** : Baseline (pas d'acc√©l√©ration)

**Configuration manuelle** (.env):
```bash
EMBEDDING_DEVICE=cpu
EMBEDDING_BATCH=16
EMBEDDING_FP16=false
EMBEDDING_MAX_LEN=4096
EMBEDDING_CACHE_SIZE=1000

RERANK_DEVICE=cpu
RERANK_ENABLE=true
RERANK_K=5
RERANK_FINAL_N=3
```

---

## Advanced Optimizations

### 1. Quantization (Exp√©rimental)

La quantization INT8 peut r√©duire l'utilisation m√©moire de ~4x et acc√©l√©rer l'inf√©rence de 2-3x.

**‚ö†Ô∏è Note** : Perte de qualit√© l√©g√®re (~1-2% de pr√©cision)

```python
# √Ä ajouter dans services/bge.py
import torch.quantization

def _load_model_quantized():
    model = BGEM3FlagModel(DEFAULT_MODEL, devices=[DEFAULT_DEVICE], use_fp16=False)

    # Quantization dynamique (compatible CPU/CUDA/MPS)
    model.model = torch.quantization.quantize_dynamic(
        model.model,
        {torch.nn.Linear},  # Quantize linear layers
        dtype=torch.qint8
    )

    return model
```

### 2. Model Compilation (PyTorch 2.0+)

`torch.compile()` peut acc√©l√©rer les mod√®les de 30-50% suppl√©mentaire.

**‚ö†Ô∏è Note** : N√©cessite PyTorch 2.0+ et peut augmenter le temps de d√©marrage

```python
# √Ä ajouter dans services/bge.py
def _load_model_compiled():
    model = BGEM3FlagModel(DEFAULT_MODEL, devices=[DEFAULT_DEVICE], use_fp16=use_fp16)

    # Compile le mod√®le avec PyTorch 2.0+
    if hasattr(torch, 'compile'):
        model.model = torch.compile(model.model, mode="reduce-overhead")

    return model
```

### 3. Cache Optimizations

Le cache LRU est d√©j√† impl√©ment√© pour les queries. Pour am√©liorer :

```bash
# Augmenter la taille du cache si vous avez beaucoup de queries r√©p√©t√©es
EMBEDDING_CACHE_SIZE=5000

# D√©sactiver le cache si vos queries ne se r√©p√®tent jamais
EMBEDDING_CACHE_SIZE=0
```

### 4. Batch Size Tuning

Trouvez le batch size optimal pour votre GPU :

```bash
# Augmentez progressivement jusqu'√† OOM (Out Of Memory)
EMBEDDING_BATCH=16   # Safe
EMBEDDING_BATCH=32   # Mac Silicon optimal
EMBEDDING_BATCH=64   # CUDA optimal
EMBEDDING_BATCH=128  # High-end GPU
EMBEDDING_BATCH=256  # Very high-end GPU
```

**Comment trouver l'optimal** :
```bash
# Testez diff√©rentes valeurs
for batch in 16 32 64 128; do
  export EMBEDDING_BATCH=$batch
  python benchmark_embeddings.py 2>&1 | grep "Average"
done
```

### 5. R√©duire la latence de d√©marrage

Les mod√®les BGE sont lourds (~2GB). Pour r√©duire le temps de chargement :

1. **Pr√©charger les mod√®les au d√©marrage** :
   ```python
   # Dans server/main.py, ajouter dans startup()
   @app.on_event("startup")
   async def startup():
       global datastore
       datastore = await get_datastore()

       # Pr√©charge les mod√®les
       from services.bge import _load_model
       from services.rerank import _get
       _load_model()  # Charge BGE-M3
       _get()         # Charge Reranker
   ```

2. **Utiliser un model server persistant** (comme vLLM ou Text Embeddings Inference)

---

## Benchmarking

### Running Benchmarks

```bash
# Benchmark complet
python benchmark_embeddings.py

# Benchmark avec profiling
python -m cProfile -o profile.stats benchmark_embeddings.py

# Analyser le profiling
python -c "import pstats; p = pstats.Stats('profile.stats'); p.sort_stats('cumulative').print_stats(20)"
```

### Interpr√©tation des r√©sultats

**M√©triques importantes** :
- **Average Time** : Temps moyen par op√©ration
- **Throughput** : Nombre d'op√©rations par seconde
- **Std Dev** : Stabilit√© (plus faible = mieux)

**Benchmarks typiques** (Mac M1):
```
Test                                     Avg Time    Throughput
----------------------------------------------------------------------
Single query embedding (short)              0.050s  20.00 queries/s
10 queries embedding                        0.450s  22.22 queries/s
20 documents embedding                      0.300s  66.67 docs/s
Rerank 5 passages                           0.080s  62.50 ops/s
```

---

## Troubleshooting

### Probl√®me : MPS non disponible sur Mac

**Sympt√¥me** : `torch.backends.mps.is_available()` retourne `False`

**Solution** :
```bash
# V√©rifier version PyTorch
python -c "import torch; print(torch.__version__)"

# Doit √™tre >= 1.12.0 pour MPS
pip3 install --upgrade torch
```

### Probl√®me : CUDA Out Of Memory

**Sympt√¥me** : `RuntimeError: CUDA out of memory`

**Solutions** :
1. R√©duire `EMBEDDING_BATCH`
2. R√©duire `EMBEDDING_MAX_LEN`
3. Activer gradient checkpointing (avanc√©)
4. Utiliser un GPU avec plus de VRAM

```bash
# Monitorer l'utilisation GPU
watch -n 1 nvidia-smi  # CUDA
# ou
sudo powermetrics --samplers gpu_power -i 1000  # Mac
```

### Probl√®me : Performance d√©grad√©e apr√®s optimisation

**Causes possibles** :
1. FP16 instable sur votre plateforme
2. Batch size trop grand (thrashing m√©moire)
3. MPS fallback vers CPU

**Diagnostic** :
```bash
# Activer logs d√©taill√©s
export PYTORCH_MPS_LOG_LEVEL=DEBUG  # Mac
export CUDA_LAUNCH_BLOCKING=1       # CUDA

# V√©rifier device r√©ellement utilis√©
python -c "from services.bge import DEFAULT_DEVICE; print(f'Device: {DEFAULT_DEVICE}')"
```

### Probl√®me : Erreurs d'import FlagEmbedding

**Sympt√¥me** : `ModuleNotFoundError: No module named 'FlagEmbedding'`

**Solution** :
```bash
pip3 install -U FlagEmbedding
```

---

## Performance Comparison Table

| Platform          | Device | Speedup | Batch | FP16  | Recommended Use Case          |
|-------------------|--------|---------|-------|-------|-------------------------------|
| Mac M1/M2/M3      | MPS    | 5-10x   | 32    | No    | Development, prototyping      |
| NVIDIA RTX 3090   | CUDA   | 30-50x  | 128   | Yes   | Production, large datasets    |
| NVIDIA T4         | CUDA   | 10-20x  | 64    | Yes   | Cloud deployment              |
| Intel/AMD CPU     | CPU    | 1x      | 16    | No    | Fallback only                 |

---

## Additional Resources

- [FlagEmbedding Documentation](https://github.com/FlagOpen/FlagEmbedding)
- [PyTorch MPS Backend](https://pytorch.org/docs/stable/notes/mps.html)
- [CUDA Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)
- [BGE Models on HuggingFace](https://huggingface.co/BAAI)

---

## Support

Si vous rencontrez des probl√®mes :
1. V√©rifier les logs avec `RUST_LOG=debug` ou √©quivalent
2. Tester avec `python optimize_platform.py`
3. Ex√©cuter `python benchmark_embeddings.py` pour diagnostics
4. Ouvrir une issue avec les logs et config

---

**Derni√®re mise √† jour** : 2025-01-13
